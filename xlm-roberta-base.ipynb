{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install dependencies","metadata":{}},{"cell_type":"code","source":"pip install --retries 0 transformers scikit-learn torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:31:53.721723Z","iopub.execute_input":"2025-12-03T10:31:53.721914Z","iopub.status.idle":"2025-12-03T10:33:03.377199Z","shell.execute_reply.started":"2025-12-03T10:31:53.721897Z","shell.execute_reply":"2025-12-03T10:33:03.376366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:33:03.380802Z","iopub.execute_input":"2025-12-03T10:33:03.381036Z","iopub.status.idle":"2025-12-03T10:33:13.651412Z","shell.execute_reply.started":"2025-12-03T10:33:03.381007Z","shell.execute_reply":"2025-12-03T10:33:13.650655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Supress unnecessary warnings","metadata":{}},{"cell_type":"markdown","source":"This controls the verbosity of TensorFlow’s C++ backend logging. The setting (\"3\") hides everything except FATAL errors. \n\nSecond line configures the behavior of XLA, a TensorFlow compiler that optimizes computations. This setting may help silence certain XLA-related messages or ensure consistent execution, especially in environments like Jupyter or debugging sessions.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # For TensorFlow noise\nos.environ[\"XLA_FLAGS\"] = \"--xla_cpu_multi_thread_eigen=false\"  # May suppress some XLA messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:33:18.22204Z","iopub.execute_input":"2025-12-03T10:33:18.222699Z","iopub.status.idle":"2025-12-03T10:33:18.226579Z","shell.execute_reply.started":"2025-12-03T10:33:18.222668Z","shell.execute_reply":"2025-12-03T10:33:18.225943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom transformers import XLMRobertaTokenizer, XLMRobertaModel\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport re\nimport unicodedata\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:53:41.520513Z","iopub.execute_input":"2025-12-03T11:53:41.52107Z","iopub.status.idle":"2025-12-03T11:53:41.525699Z","shell.execute_reply.started":"2025-12-03T11:53:41.521047Z","shell.execute_reply":"2025-12-03T11:53:41.524986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPU or CPU","metadata":{}},{"cell_type":"markdown","source":"This block of code loads a pre-trained XLM-RoBERTa model and moves it to the appropriate device (GPU if available, otherwise CPU).","metadata":{}},{"cell_type":"code","source":"model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:57:08.506671Z","iopub.execute_input":"2025-12-03T10:57:08.507385Z","iopub.status.idle":"2025-12-03T10:57:12.976412Z","shell.execute_reply.started":"2025-12-03T10:57:08.507361Z","shell.execute_reply":"2025-12-03T10:57:12.975626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"markdown","source":"This block sets up configuration parameters for training or fine-tuning a DL model, likely using XLM-RoBERTa for a text classification task.\n* Sets the random seed for reproducibility.\n* Specifies the pretrained model to use (from Hugging Face Transformers).\n* Sets the maximum sequence length for tokenized input texts.\n* Sets the number of samples processed at once during training/inference.\n* Number of complete passes through the training dataset.\n* Sets the learning rate for the optimizer.\n* The number of output classes in your classification task.\n* The number of initial transformer layers to freeze (i.e., not update during training).\n* Sets the device (GPU if available, otherwise CPU) for all model computations.","metadata":{}},{"cell_type":"code","source":"# Config\nSEED = 42\nMODEL_NAME = \"xlm-roberta-base\"\nMAX_LENGTH = 512\nBATCH_SIZE = 64\nEPOCHS = 10\nLR = 0.0001\nNUM_CLASSES = 3\nFREEZE_LAYERS = 10\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:57:18.155536Z","iopub.execute_input":"2025-12-03T10:57:18.156102Z","iopub.status.idle":"2025-12-03T10:57:18.159944Z","shell.execute_reply.started":"2025-12-03T10:57:18.156081Z","shell.execute_reply":"2025-12-03T10:57:18.1593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set Seed","metadata":{}},{"cell_type":"markdown","source":"This block sets random seeds and configures PyTorch's backend to ensure deterministic and reproducible results across multiple runs.","metadata":{}},{"cell_type":"code","source":"# Set seeds\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:57:21.244385Z","iopub.execute_input":"2025-12-03T10:57:21.244663Z","iopub.status.idle":"2025-12-03T10:57:21.249912Z","shell.execute_reply.started":"2025-12-03T10:57:21.244642Z","shell.execute_reply":"2025-12-03T10:57:21.249203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clean and pre-process text function","metadata":{}},{"cell_type":"markdown","source":"This function clean_text is designed to preprocess and clean raw text data, often as part of a natural language processing (NLP) pipeline. It takes a dictionary example (commonly used with Hugging Face datasets), cleans the \"text\" field, and returns the modified dictionary.","metadata":{}},{"cell_type":"code","source":"# Clean text \ndef clean_text(example):\n    text = example[\"text\"]\n    text = unicodedata.normalize(\"NFKC\", text)\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s.,!?¿¡]+\", \"\", text, flags=re.UNICODE)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    example[\"text\"] = text\n    return example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:57:33.398322Z","iopub.execute_input":"2025-12-03T10:57:33.398587Z","iopub.status.idle":"2025-12-03T10:57:34.756903Z","shell.execute_reply.started":"2025-12-03T10:57:33.398571Z","shell.execute_reply":"2025-12-03T10:57:34.756071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load train and validation datasets","metadata":{}},{"cell_type":"markdown","source":"This code block loads the multilingual sentiment dataset, filters it by language, and prepares fixed-size English, French, and Chinese subsets for training and validation.","metadata":{}},{"cell_type":"code","source":"# Load English training dataset\ndataset = load_dataset(\"clapAI/MultiLingualSentiment\")\nenglish_data = dataset[\"train\"].filter(lambda x: x[\"language\"] == \"en\")\nenglish_data_shuffled = english_data.shuffle(seed=42)\nenglish_sample = english_data_shuffled.select(range(10000))\nenglish_list = english_sample.to_list()\nenglish_labels = [item[\"label\"] for item in english_list]\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:57:42.128784Z","iopub.execute_input":"2025-12-03T10:57:42.129305Z","iopub.status.idle":"2025-12-03T10:58:20.022779Z","shell.execute_reply.started":"2025-12-03T10:57:42.129281Z","shell.execute_reply":"2025-12-03T10:58:20.022114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load English validation dataset\nenglish_data_val = dataset[\"validation\"].filter(lambda x: x[\"language\"] == \"en\")\nenglish_data_val_shuffled = english_data_val.shuffle(seed=42)\nenglish_sample_val = english_data_val_shuffled.select(range(1250))\nenglish_list_val = english_sample_val.to_list()\nenglish_labels_val = [item[\"label\"] for item in english_list_val]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:58:24.575629Z","iopub.execute_input":"2025-12-03T10:58:24.576134Z","iopub.status.idle":"2025-12-03T10:58:27.109077Z","shell.execute_reply.started":"2025-12-03T10:58:24.576109Z","shell.execute_reply":"2025-12-03T10:58:27.108374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load French validation dataset\nfrench_data = dataset[\"validation\"].filter(lambda x: x[\"language\"] == \"fr\")\nfrench_data_shuffled = french_data.shuffle(seed=42)\nfrench_sample = french_data_shuffled.select(range(1250))\nfrench_list = french_sample.to_list()\nfrench_labels = [item[\"label\"] for item in french_list]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:58:29.90026Z","iopub.execute_input":"2025-12-03T10:58:29.900528Z","iopub.status.idle":"2025-12-03T10:58:32.372713Z","shell.execute_reply.started":"2025-12-03T10:58:29.900508Z","shell.execute_reply":"2025-12-03T10:58:32.372194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Chinese validation dataset\nchinese_data = dataset[\"validation\"].filter(lambda x: x[\"language\"] == \"zh\")\nchinese_data_shuffled = chinese_data.shuffle(seed=42)\nchinese_sample = chinese_data_shuffled.select(range(1250))\nchinese_list = chinese_sample.to_list()\nchinese_labels = [item[\"label\"] for item in chinese_list]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:58:39.549075Z","iopub.execute_input":"2025-12-03T10:58:39.549765Z","iopub.status.idle":"2025-12-03T10:58:42.0488Z","shell.execute_reply.started":"2025-12-03T10:58:39.549744Z","shell.execute_reply":"2025-12-03T10:58:42.048293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"markdown","source":"This code tokenizes our cleaned text data using XLM-RoBERTa, formats it for PyTorch, and wraps it into dataloaders for training and validation. \n* Loads the XLM-RoBERTa tokenizer (from Hugging Face).\n* Tokenizes a batch of text\n* Applies the tokenization to Hugging Face Dataset objects.\n* Converts the tokenized dataset into PyTorch tensors.\n* Wraps datasets in PyTorch DataLoader objects.","metadata":{}},{"cell_type":"code","source":"# Convert lists to HuggingFace Dataset objects\ntrain_dataset = Dataset.from_list(english_list)\nval_dataset_en = Dataset.from_list(english_list_val)\nval_dataset_fr = Dataset.from_list(french_list)\nval_dataset_zh = Dataset.from_list(chinese_list)\n\n# Tokenization function using a pretrained tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\ndef tokenize_batch(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n\n# Apply the tokenizer to each dataset\ntrain_dataset = train_dataset.map(tokenize_batch, batched=True)\nval_dataset_en = val_dataset_en.map(tokenize_batch, batched=True)\nval_dataset_fr = val_dataset_fr.map(tokenize_batch, batched=True)\nval_dataset_zh = val_dataset_zh.map(tokenize_batch, batched=True)\n\n# Set dataset format for PyTorch (tensors)\ncolumns = [\"input_ids\", \"attention_mask\", \"label\"]\ntrain_dataset.set_format(type=\"torch\", columns=columns)\nval_dataset_en.set_format(type=\"torch\", columns=columns)\nval_dataset_fr.set_format(type=\"torch\", columns=columns)\nval_dataset_zh.set_format(type=\"torch\", columns=columns)\n\n# Create DataLoaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader_en = DataLoader(val_dataset_en, batch_size=BATCH_SIZE)\nval_loader_fr = DataLoader(val_dataset_fr, batch_size=BATCH_SIZE)\nval_loader_zh = DataLoader(val_dataset_zh, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:02:29.110653Z","iopub.execute_input":"2025-12-03T11:02:29.111254Z","iopub.status.idle":"2025-12-03T11:02:40.82335Z","shell.execute_reply.started":"2025-12-03T11:02:29.111228Z","shell.execute_reply":"2025-12-03T11:02:40.82262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load pre-trained model XLM RoBERTa","metadata":{}},{"cell_type":"markdown","source":"We freeze the lower layers (the first N) and allow the upper layers to fine-tune, since the bottom ones capture more general linguistic features.","metadata":{}},{"cell_type":"code","source":"# Load pretrained model \nbase_model = XLMRobertaModel.from_pretrained(MODEL_NAME)\nfor i, layer in enumerate(base_model.encoder.layer):\n    if i >= FREEZE_LAYERS:\n        for param in layer.parameters():\n            param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:04:28.898732Z","iopub.execute_input":"2025-12-03T11:04:28.899Z","iopub.status.idle":"2025-12-03T11:04:29.155955Z","shell.execute_reply.started":"2025-12-03T11:04:28.898981Z","shell.execute_reply":"2025-12-03T11:04:29.1552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification model","metadata":{}},{"cell_type":"markdown","source":"This code defines and initializes a custom classification model for sentiment analysis using a pre-trained XLM-RoBERTa encoder and a linear classifier head.","metadata":{}},{"cell_type":"code","source":"# Classification model \nclass SentimentClassifier(nn.Module):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        self.classifier = nn.Linear(encoder.config.hidden_size, NUM_CLASSES)\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():\n            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n            cls_token = outputs.last_hidden_state[:, 0, :]\n        logits = self.classifier(cls_token)\n        return logits\n\nmodel = SentimentClassifier(base_model).to(DEVICE)\noptimizer = torch.optim.Adam(model.classifier.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_losses, val_losses, train_accs, val_accs = [], [], [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:04:38.410065Z","iopub.execute_input":"2025-12-03T11:04:38.410759Z","iopub.status.idle":"2025-12-03T11:04:38.800121Z","shell.execute_reply.started":"2025-12-03T11:04:38.410736Z","shell.execute_reply":"2025-12-03T11:04:38.799577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Validation","metadata":{}},{"cell_type":"markdown","source":"This block implements a training and validation loop for the SentimentClassifier model over multiple epochs. It performs model optimization, accuracy tracking, and periodic logging.","metadata":{}},{"cell_type":"code","source":"label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n\ntrain_losses, train_accs = [], []\n\nval_losses_en, val_accs_en = [], []\nval_losses_fr, val_accs_fr = [], []\nval_losses_zh, val_accs_zh = [], []\n\nfor epoch in range(EPOCHS):\n    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n\n    for i, batch in enumerate(train_loader, 1):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = torch.tensor([label_map[label] for label in batch[\"label\"]]).to(DEVICE)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = torch.argmax(logits, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += loss.item()\n\n        if i % 1000 == 0 or i == len(train_loader):\n            print(f\"  Batch {i}/{len(train_loader)}: Loss = {loss.item():.4f}\")\n\n    epoch_train_loss = total_loss / len(train_loader)\n    epoch_train_acc = correct / total\n    train_losses.append(epoch_train_loss)\n    train_accs.append(epoch_train_acc)\n\n    print(f\"Training: Loss = {epoch_train_loss:.4f}, Accuracy = {epoch_train_acc:.4f}\")\n\n    # Validation function\n    def evaluate(loader, label_map):\n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for batch in loader:\n                input_ids = batch[\"input_ids\"].to(DEVICE)\n                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n                labels = torch.tensor([label_map[label] for label in batch[\"label\"]]).to(DEVICE)\n\n                logits = model(input_ids, attention_mask)\n                loss = criterion(logits, labels)\n                val_loss += loss.item()\n\n                preds = torch.argmax(logits, dim=1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n\n        return val_loss / len(loader), val_correct / val_total\n\n    val_loss_en, val_acc_en = evaluate(val_loader_en, label_map)\n    val_losses_en.append(val_loss_en)\n    val_accs_en.append(val_acc_en)\n\n    val_loss_fr, val_acc_fr = evaluate(val_loader_fr, label_map)\n    val_losses_fr.append(val_loss_fr)\n    val_accs_fr.append(val_acc_fr)\n\n    val_loss_zh, val_acc_zh = evaluate(val_loader_zh, label_map)\n    val_losses_zh.append(val_loss_zh)\n    val_accs_zh.append(val_acc_zh)\n\n    print(f\"Validation EN: Loss = {val_loss_en:.4f}, Accuracy = {val_acc_en:.4f}\")\n    print(f\"Validation FR: Loss = {val_loss_fr:.4f}, Accuracy = {val_acc_fr:.4f}\")\n    print(f\"Validation ZH: Loss = {val_loss_zh:.4f}, Accuracy = {val_acc_zh:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:04:47.558445Z","iopub.execute_input":"2025-12-03T11:04:47.559089Z","iopub.status.idle":"2025-12-03T11:41:02.371082Z","shell.execute_reply.started":"2025-12-03T11:04:47.559066Z","shell.execute_reply":"2025-12-03T11:41:02.370438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification report","metadata":{}},{"cell_type":"code","source":"# Generate classification report\ndef generate_report(val_loader, label_map, lang_code=\"English\"):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = torch.tensor([label_map[label] for label in batch[\"label\"]]).to(DEVICE)\n            logits = model(input_ids, attention_mask)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    target_names = [\"Negative\", \"Neutral\", \"Positive\"]\n    report = classification_report(all_labels, all_preds, target_names=target_names, digits=4)\n    print(f\"\\nClassification Report ({lang_code}):\\n\")\n    print(report)\n\n# Generate reports for all three languages\ngenerate_report(val_loader_en, label_map, \"English\")\ngenerate_report(val_loader_fr, label_map, \"French\")\ngenerate_report(val_loader_zh, label_map, \"Chinese\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:53:55.246073Z","iopub.execute_input":"2025-12-03T11:53:55.246452Z","iopub.status.idle":"2025-12-03T11:54:52.132751Z","shell.execute_reply.started":"2025-12-03T11:53:55.246429Z","shell.execute_reply":"2025-12-03T11:54:52.132065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plot loss and accuracy","metadata":{}},{"cell_type":"code","source":"# Plot Loss & Accuracy \nepochs_range = range(1, EPOCHS + 1)\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, train_losses, label=\"Train Loss\")\nplt.plot(epochs_range, val_losses_en, label=\"Val Loss (EN)\")\nplt.plot(epochs_range, val_losses_fr, label=\"Val Loss (FR)\")\nplt.plot(epochs_range, val_losses_zh, label=\"Val Loss (ZH)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Curve\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_accs, label=\"Train Accuracy\")\nplt.plot(epochs_range, val_accs_en, label=\"Val Acc (EN)\")\nplt.plot(epochs_range, val_accs_fr, label=\"Val Acc (FR)\")\nplt.plot(epochs_range, val_accs_zh, label=\"Val Acc (ZH)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy Curve\")\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(\"loss_accuracy_curve_multilingual.png\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:03.521622Z","iopub.execute_input":"2025-12-03T11:48:03.522354Z","iopub.status.idle":"2025-12-03T11:48:04.251252Z","shell.execute_reply.started":"2025-12-03T11:48:03.522332Z","shell.execute_reply":"2025-12-03T11:48:04.250468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion matrix","metadata":{}},{"cell_type":"markdown","source":"This code block evaluates the trained sentiment classifier on the validation set by computing predictions, metrics (accuracy and F1), and plotting a confusion matrix to visualize how well the model performed per class.","metadata":{}},{"cell_type":"code","source":"def evaluate_model(val_loader, label_map, lang_code=\"English\"):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = torch.tensor([label_map[label] for label in batch[\"label\"]]).to(DEVICE)\n            logits = model(input_ids, attention_mask)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    print(f\"\\nXLM-R Val ({lang_code}) → Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Neg\", \"Neu\", \"Pos\"], yticklabels=[\"Neg\", \"Neu\", \"Pos\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(f\"Confusion Matrix ({lang_code})\")\n    plt.tight_layout()\n    plt.savefig(f\"confusion_matrix_{lang_code.lower()}.png\")\n    plt.show()\n\n# Call the evaluation for each language\nevaluate_model(val_loader_en, label_map, \"English\")\nevaluate_model(val_loader_fr, label_map, \"French\")\nevaluate_model(val_loader_zh, label_map, \"Chinese\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:19.15819Z","iopub.execute_input":"2025-12-03T11:48:19.158505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Further analysis","metadata":{}},{"cell_type":"markdown","source":"This code block visualizes the training and validation data distributions to examine their potential impact on classification accuracy.","metadata":{}},{"cell_type":"code","source":"def get_distribution(data, labels=[\"negative\", \"neutral\", \"positive\"]):\n    counter = Counter([item[\"label\"] for item in data])\n    total = sum(counter.values())\n    return [counter.get(label, 0) / total for label in labels]\n\ngroups = [\"English Train\", \"English Val\", \"French Val\", \"Chinese Val\"]\ndatasets = [english_list, english_list_val, french_list, chinese_list]\nlabel_names = [\"negative\", \"neutral\", \"positive\"]\n\ndistributions = [get_distribution(data) for data in datasets]\n\nbar_width = 0.2\nx = np.arange(len(groups))\n\nplt.figure(figsize=(10, 6))\n\nfor i in range(len(label_names)):\n    label = label_names[i]\n    values = [dist[i] for dist in distributions]\n    positions = x + (i - 1) * bar_width\n    bars = plt.bar(positions, values, width=bar_width, label=label)\n    for pos, val in zip(positions, values):\n        plt.text(pos, val + 0.01, f\"{val*100:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n\nplt.xticks(x, groups)\nplt.ylim(0, 1)\nplt.ylabel(\"Proportion\")\nplt.title(\"Sentiment Label Distribution within Each Dataset\")\nplt.legend(title=\"Sentiment\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:55:33.700431Z","iopub.execute_input":"2025-12-03T11:55:33.700925Z","iopub.status.idle":"2025-12-03T11:55:33.922654Z","shell.execute_reply.started":"2025-12-03T11:55:33.700903Z","shell.execute_reply":"2025-12-03T11:55:33.921893Z"}},"outputs":[],"execution_count":null}]}