{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\nimport re\nimport spacy\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModel\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:01.135431Z","iopub.execute_input":"2025-12-03T14:22:01.135687Z","iopub.status.idle":"2025-12-03T14:22:16.672332Z","shell.execute_reply.started":"2025-12-03T14:22:01.135654Z","shell.execute_reply":"2025-12-03T14:22:16.671521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization functions","metadata":{}},{"cell_type":"code","source":"_patterns = [r\"\\'\", r\"\\\"\", r\"\\.\", r\"<br \\/>\", r\",\", r\"\\(\", r\"\\)\", r\"\\!\", r\"\\?\", r\"\\;\", r\"\\:\", r\"\\s+\"]\n\n_replacements = [\" '  \", \"\", \" . \", \" \", \" , \", \" ( \", \" ) \", \" ! \", \" ? \", \" \", \" \", \" \"]\n\n_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n\ndef _basic_english_normalize(line):\n    r\"\"\"\n    Basic normalization for a line of text.\n    Normalization includes\n    - lowercasing\n    - complete some basic text normalization for English words as follows:\n        add spaces before and after '\\''\n        remove '\\\"',\n        add spaces before and after '.'\n        replace '<br \\/>'with single space\n        add spaces before and after ','\n        add spaces before and after '('\n        add spaces before and after ')'\n        add spaces before and after '!'\n        add spaces before and after '?'\n        replace ';' with single space\n        replace ':' with single space\n        replace multiple spaces with single space\n\n    Returns a list of tokens after splitting on whitespace.\n    \"\"\"\n\n    line = line.lower()\n    for pattern_re, replaced_str in _patterns_dict:\n        line = pattern_re.sub(replaced_str, line)\n    return line.split()\n\n\n\ndef get_tokenizer(tokenizer, language='en'):\n    r\"\"\"\n    Generate tokenizer function for a string sentence.\n\n    Arguments:\n        tokenizer: the name of tokenizer function. If None, it returns split()\n            function, which splits the string sentence by space.\n            If basic_english, it returns _basic_english_normalize() function,\n            which normalize the string first and split by space. If a callable\n            function, it will return the function. If a tokenizer library\n            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n            corresponding library.\n        language: Default en\n\n    Examples:\n        >>> import torchtext\n        >>> from torchtext.data import get_tokenizer\n        >>> tokenizer = get_tokenizer(\"basic_english\")\n        >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n        >>> tokens\n        >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n\n    \"\"\"\n\n    # default tokenizer is string.split(), added as a module function for serialization\n\n\n    if tokenizer == \"basic_english\":\n        if language != 'en':\n            raise ValueError(\"Basic normalization is only available for Enlish(en)\")\n        return _basic_english_normalize\n\n    # simply return if a function is passed\n    if callable(tokenizer):\n        return tokenizer\n\n    if tokenizer == \"spacy\":\n        try:\n            import spacy\n            spacy = spacy.load(language)\n            return partial(_spacy_tokenize, spacy=spacy)\n        except ImportError:\n            print(\"Please install SpaCy. \"\n                  \"See the docs at https://spacy.io for more information.\")\n            raise\n        except AttributeError:\n            print(\"Please install SpaCy and the SpaCy {} tokenizer. \"\n                  \"See the docs at https://spacy.io for more \"\n                  \"information.\".format(language))\n            raise\n    elif tokenizer == \"moses\":\n        try:\n            from sacremoses import MosesTokenizer\n            moses_tokenizer = MosesTokenizer()\n            return moses_tokenizer.tokenize\n        except ImportError:\n            print(\"Please install SacreMoses. \"\n                  \"See the docs at https://github.com/alvations/sacremoses \"\n                  \"for more information.\")\n            raise\n    elif tokenizer == \"toktok\":\n        try:\n            from nltk.tokenize.toktok import ToktokTokenizer\n            toktok = ToktokTokenizer()\n            return toktok.tokenize\n        except ImportError:\n            print(\"Please install NLTK. \"\n                  \"See the docs at https://nltk.org  for more information.\")\n            raise\n    elif tokenizer == 'revtok':\n        try:\n            import revtok\n            return revtok.tokenize\n        except ImportError:\n            print(\"Please install revtok.\")\n            raise\n    elif tokenizer == 'subword':\n        try:\n            import revtok\n            return partial(revtok.tokenize, decap=True)\n        except ImportError:\n            print(\"Please install revtok.\")\n            raise\n    raise ValueError(\"Requested tokenizer {}, valid choices are a \"\n                     \"callable that takes a single string as input, \"\n                     \"\\\"revtok\\\" for the revtok reversible tokenizer, \"\n                     \"\\\"subword\\\" for the revtok caps-aware tokenizer, \"\n                     \"\\\"spacy\\\" for the SpaCy English tokenizer, or \"\n                     \"\\\"moses\\\" for the NLTK port of the Moses tokenization \"\n                     \"script.\".format(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:20.85679Z","iopub.execute_input":"2025-12-03T14:22:20.857659Z","iopub.status.idle":"2025-12-03T14:22:20.866397Z","shell.execute_reply.started":"2025-12-03T14:22:20.857625Z","shell.execute_reply":"2025-12-03T14:22:20.865775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set random seed","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:28.094029Z","iopub.execute_input":"2025-12-03T14:22:28.094775Z","iopub.status.idle":"2025-12-03T14:22:28.103922Z","shell.execute_reply.started":"2025-12-03T14:22:28.094748Z","shell.execute_reply":"2025-12-03T14:22:28.103295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set device","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:32.31825Z","iopub.execute_input":"2025-12-03T14:22:32.318931Z","iopub.status.idle":"2025-12-03T14:22:32.32327Z","shell.execute_reply.started":"2025-12-03T14:22:32.318902Z","shell.execute_reply":"2025-12-03T14:22:32.322695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load datasets","metadata":{}},{"cell_type":"code","source":"# Load the MultilingualSentiment dataset\ndataset = load_dataset(\"clapAI/MultiLingualSentiment\")\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:37.532932Z","iopub.execute_input":"2025-12-03T14:22:37.533487Z","iopub.status.idle":"2025-12-03T14:22:52.414077Z","shell.execute_reply.started":"2025-12-03T14:22:37.53346Z","shell.execute_reply":"2025-12-03T14:22:52.41342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a small number of English samples as training dataset\nenglish_train_dataset = dataset['train'].filter(lambda example: example['language'] == 'en')\nenglish_shuffled_dataset = english_train_dataset.shuffle(seed=42)\nenglish_sampled_dataset = english_shuffled_dataset.select(range(10000))\n# english_sampled_dataset.select(range(5)).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:56.192129Z","iopub.execute_input":"2025-12-03T14:22:56.192906Z","iopub.status.idle":"2025-12-03T14:23:17.245861Z","shell.execute_reply.started":"2025-12-03T14:22:56.192878Z","shell.execute_reply":"2025-12-03T14:23:17.245239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate French validation dataset\nval_fr = dataset['validation'].filter(lambda x: x['language'] == 'fr')\nval_fr_shuffled = val_fr.shuffle(seed=42)\nval_fr_sampled = val_fr_shuffled.select(range(1250))\n# val_fr_sampled.select(range(50)).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:17.247234Z","iopub.execute_input":"2025-12-03T14:23:17.247856Z","iopub.status.idle":"2025-12-03T14:23:19.81645Z","shell.execute_reply.started":"2025-12-03T14:23:17.247835Z","shell.execute_reply":"2025-12-03T14:23:19.81575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate English validation dataset\nval_en = dataset['validation'].filter(lambda x: x['language'] == 'en')\nval_en_shuffled = val_en.shuffle(seed=42)\nval_en_sampled = val_en_shuffled.select(range(1250))\n# val_en_sampled.select(range(5)).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:21.813211Z","iopub.execute_input":"2025-12-03T14:23:21.814004Z","iopub.status.idle":"2025-12-03T14:23:24.383417Z","shell.execute_reply.started":"2025-12-03T14:23:21.813977Z","shell.execute_reply":"2025-12-03T14:23:24.382864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Chinese validation dataset\nval_zh = dataset['validation'].filter(lambda x: x['language'] == 'zh')\nval_zh_shuffled = val_zh.shuffle(seed=42)\nval_zh_sampled = val_zh_shuffled.select(range(1250))\n# val_zh_sampled.select(range(5)).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:24.674638Z","iopub.execute_input":"2025-12-03T14:23:24.674898Z","iopub.status.idle":"2025-12-03T14:23:27.208168Z","shell.execute_reply.started":"2025-12-03T14:23:24.67488Z","shell.execute_reply":"2025-12-03T14:23:27.207644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate French test dataset\ntest_fr = dataset['test'].filter(lambda x: x['language'] == 'fr')\ntest_fr_shuffled = test_fr.shuffle(seed=42)\ntest_fr_sampled = test_fr_shuffled.select(range(1250))\n# test_fr_sampled.select(range(5)).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:28.034878Z","iopub.execute_input":"2025-12-03T14:23:28.035153Z","iopub.status.idle":"2025-12-03T14:23:30.584204Z","shell.execute_reply.started":"2025-12-03T14:23:28.035132Z","shell.execute_reply":"2025-12-03T14:23:30.583607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training data (English training dataset)\nx_train_text = english_sampled_dataset[\"text\"]  # Training input texts in English\ny_train = english_sampled_dataset[\"label\"]     # Corresponding training labels\n\n# Validation data (French validation dataset)\nx_val_fr_text = val_fr_sampled[\"text\"]  # Validation input texts in French\ny_val_fr = val_fr_sampled[\"label\"]      # Corresponding French validation labels\n\n# Validation data (English validation dataset)\nx_val_en_text = val_en_sampled[\"text\"]  # Validation input texts in English\ny_val_en = val_en_sampled[\"label\"]      # Corresponding English validation labels\n\n# Validation data (Chinese validation dataset)\nx_val_zh_text = val_zh_sampled[\"text\"]  # Validation input texts in Chinese\ny_val_zh = val_zh_sampled[\"label\"]      # Corresponding Chinese validation labels\n\n# Test data (e.g. French test dataset, unchanged)\nx_test_text = test_fr_sampled[\"text\"]  # Test input texts in French\ny_test = test_fr_sampled[\"label\"]      # Corresponding test labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:33.589151Z","iopub.execute_input":"2025-12-03T14:23:33.589433Z","iopub.status.idle":"2025-12-03T14:23:33.596806Z","shell.execute_reply.started":"2025-12-03T14:23:33.589412Z","shell.execute_reply":"2025-12-03T14:23:33.596059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load pretrained mBERT embeddings","metadata":{}},{"cell_type":"code","source":"# Load pretrained multilingual BERT model and tokenizer\nmodel_name = \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nencoder = AutoModel.from_pretrained(model_name)\n\n# Move model to device and set to evaluation mode\nencoder = encoder.to(DEVICE)\nencoder.eval()\n\n# Freeze model parameters to avoid training\nfor param in encoder.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:45.153151Z","iopub.execute_input":"2025-12-03T14:23:45.153399Z","iopub.status.idle":"2025-12-03T14:24:12.397104Z","shell.execute_reply.started":"2025-12-03T14:23:45.153383Z","shell.execute_reply":"2025-12-03T14:24:12.396289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate sentence-level embeddings","metadata":{}},{"cell_type":"code","source":"# Define a function for sentence-level embedding generation\ndef encode_texts(texts, batch_size=32):\n    \"\"\"\n    Encode a list of texts into sentence embeddings using pretrained encoder.\n\n    Args:\n        texts (list of str): List of input sentences.\n        batch_size (int): Number of samples per batch.\n\n    Returns:\n        torch.Tensor: Tensor of shape (num_samples, hidden_size) with sentence embeddings.\n    \"\"\"\n    embeddings = []\n\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch_texts = texts[i:i+batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n\n        with torch.no_grad():\n            outputs = encoder(**inputs)\n            # Take the [CLS] token representation as the sentence embedding\n            batch_embs = outputs.last_hidden_state[:, 0, :]\n            embeddings.append(batch_embs.cpu())\n\n    return torch.cat(embeddings, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:29:09.146126Z","iopub.execute_input":"2025-12-03T14:29:09.146405Z","iopub.status.idle":"2025-12-03T14:29:09.151084Z","shell.execute_reply.started":"2025-12-03T14:29:09.146384Z","shell.execute_reply":"2025-12-03T14:29:09.150531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print types and first few elements to check label formats\nprint(f\"type(y_train): {type(y_train)}\")\nprint(f\"First 10 y_train labels: {y_train[:10]}\")\n\nprint(f\"type(y_val_fr): {type(y_val_fr)}\")\nprint(f\"First 10 y_val_fr labels: {y_val_fr[:10]}\")\n\nprint(f\"type(y_val_en): {type(y_val_en)}\")\nprint(f\"First 10 y_val_en labels: {y_val_en[:10]}\")\n\nprint(f\"type(y_val_zh): {type(y_val_zh)}\")\nprint(f\"First 10 y_val_zh labels: {y_val_zh[:10]}\")\n\nprint(f\"type(y_test): {type(y_test)}\")\nprint(f\"First 10 y_test labels: {y_test[:10]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:29:11.752952Z","iopub.execute_input":"2025-12-03T14:29:11.753229Z","iopub.status.idle":"2025-12-03T14:29:11.770259Z","shell.execute_reply.started":"2025-12-03T14:29:11.75321Z","shell.execute_reply":"2025-12-03T14:29:11.769714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define label to index mapping\nlabel2id = {\n    \"negative\": 0,\n    \"neutral\": 1,\n    \"positive\": 2\n}\n\n# Show mapping\nprint(\"Label to ID mapping:\", label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:29:19.590206Z","iopub.execute_input":"2025-12-03T14:29:19.590474Z","iopub.status.idle":"2025-12-03T14:29:19.594231Z","shell.execute_reply.started":"2025-12-03T14:29:19.590453Z","shell.execute_reply":"2025-12-03T14:29:19.593451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert string labels to integer ids using the mapping\ny_train = [label2id[label] for label in y_train]\ny_val_fr = [label2id[label] for label in y_val_fr]\ny_val_en = [label2id[label] for label in y_val_en]\ny_val_zh = [label2id[label] for label in y_val_zh]\ny_test = [label2id[label] for label in y_test]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:29:23.209547Z","iopub.execute_input":"2025-12-03T14:29:23.209831Z","iopub.status.idle":"2025-12-03T14:29:23.879901Z","shell.execute_reply.started":"2025-12-03T14:29:23.209808Z","shell.execute_reply":"2025-12-03T14:29:23.879295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate sentence-level embeddings\nX_train = encode_texts(x_train_text)\nX_val_fr = encode_texts(x_val_fr_text)\nX_val_en = encode_texts(x_val_en_text)\nX_val_zh = encode_texts(x_val_zh_text)\nX_test = encode_texts(x_test_text)\n\ny_train = torch.tensor(y_train)\ny_val_fr = torch.tensor(y_val_fr)\ny_val_en = torch.tensor(y_val_en)\ny_val_zh = torch.tensor(y_val_zh)\ny_test = torch.tensor(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:29:28.344061Z","iopub.execute_input":"2025-12-03T14:29:28.344337Z","iopub.status.idle":"2025-12-03T14:30:24.696108Z","shell.execute_reply.started":"2025-12-03T14:29:28.344316Z","shell.execute_reply":"2025-12-03T14:30:24.695207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert tensors to NumPy arrays\nX_train_np = X_train.numpy()\nX_val_fr_np = X_val_fr.numpy()\nX_val_en_np = X_val_en.numpy()\nX_val_zh_np = X_val_zh.numpy()\nX_test_np   = X_test.numpy()\n\ny_train_np = y_train.numpy()\ny_val_fr_np = y_val_fr.numpy()\ny_val_en_np = y_val_en.numpy()\ny_val_zh_np = y_val_zh.numpy()\ny_test_np   = y_test.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:31:14.22307Z","iopub.execute_input":"2025-12-03T14:31:14.223373Z","iopub.status.idle":"2025-12-03T14:31:14.228115Z","shell.execute_reply.started":"2025-12-03T14:31:14.223351Z","shell.execute_reply":"2025-12-03T14:31:14.227543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combine LR with sentence-level embeddings","metadata":{}},{"cell_type":"code","source":"# Train the model using English training set\nmodel = LogisticRegression(solver='saga', max_iter=1000)\nmodel.fit(X_train_np, y_train_np)\n\n# Predict and evaluate on French validation set\ny_pred_fr = model.predict(X_val_fr_np)\nprint(\"Validation on French:\")\nprint(classification_report(y_val_fr_np, y_pred_fr))\n\n# Predict and evaluate on English validation set\ny_pred_en = model.predict(X_val_en_np)\nprint(\"Validation on English:\")\nprint(classification_report(y_val_en_np, y_pred_en))\n\n# Predict and evaluate on Chinese validation set\ny_pred_zh = model.predict(X_val_zh_np)\nprint(\"Validation on Chinese:\")\nprint(classification_report(y_val_zh_np, y_pred_zh))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:33:12.811413Z","iopub.execute_input":"2025-12-03T14:33:12.812036Z","iopub.status.idle":"2025-12-03T14:35:18.294642Z","shell.execute_reply.started":"2025-12-03T14:33:12.81201Z","shell.execute_reply":"2025-12-03T14:35:18.293905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate token-level embeddings","metadata":{}},{"cell_type":"code","source":"# Define a function for token-level embedding generation\ndef encode_token_embeddings(texts, batch_size=32, max_len=128):\n    \"\"\"\n    Encode texts into mBERT token embeddings (last_hidden_state).\n    Returns: Tensor of shape (num_samples, max_len, hidden_dim)\n    \"\"\"\n    all_embs = []\n\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch_texts = texts[i:i+batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True,\n                           max_length=max_len).to(DEVICE)\n\n        with torch.no_grad():\n            outputs = encoder(**inputs)\n            token_embs = outputs.last_hidden_state  # [batch, max_len, 768]\n            all_embs.append(token_embs.cpu())\n\n    return torch.cat(all_embs, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:39:53.948334Z","iopub.execute_input":"2025-12-03T14:39:53.949061Z","iopub.status.idle":"2025-12-03T14:39:53.953602Z","shell.execute_reply.started":"2025-12-03T14:39:53.949034Z","shell.execute_reply":"2025-12-03T14:39:53.952926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_loader(embeddings, labels, batch_size=64, shuffle=False):\n    X_tensor = torch.tensor(embeddings, dtype=torch.float32)\n    y_tensor = torch.tensor(labels, dtype=torch.long)\n    dataset = TensorDataset(X_tensor, y_tensor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:39:56.814002Z","iopub.execute_input":"2025-12-03T14:39:56.814791Z","iopub.status.idle":"2025-12-03T14:39:56.818989Z","shell.execute_reply.started":"2025-12-03T14:39:56.814767Z","shell.execute_reply":"2025-12-03T14:39:56.818088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set RNN","metadata":{}},{"cell_type":"code","source":"class SentimentRNN_PretrainedEmbedding(nn.Module):\n    def __init__(self, no_layers, embedding_dim, hidden_dim, output_dim, drop_prob=0.1):\n        super(SentimentRNN_PretrainedEmbedding, self).__init__()\n\n        self.no_layers = no_layers\n        self.hidden_dim = hidden_dim\n        self.drop_prob = drop_prob\n\n        # No embedding layer here, input is already embedding vectors\n\n        # LSTM layers\n        self.lstm = nn.LSTM(input_size=embedding_dim,\n                            hidden_size=hidden_dim,\n                            num_layers=no_layers,\n                            batch_first=True,\n                            dropout=drop_prob,\n                            bidirectional=False)\n\n        # Dropout layer\n        self.dropout = nn.Dropout(drop_prob)\n\n        # Fully connected output layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, hidden=None):\n        # x shape: [batch_size, seq_len, embedding_dim]\n\n        # If no hidden state is given, initialize it\n        if hidden is None:\n            batch_size = x.size(0)\n            hidden = self.init_hidden(batch_size)\n\n        lstm_out, hidden = self.lstm(x, hidden)  # lstm_out shape: [batch, seq_len, hidden_dim]\n\n        lstm_out = self.dropout(lstm_out)\n\n        last_hidden = lstm_out[:, -1, :].contiguous()  # last time step output\n\n        out = self.dropout(last_hidden)\n        out = self.fc(out)\n\n        return out, hidden\n\n    def init_hidden(self, batch_size, device=None):\n        if device is None:\n            device = next(self.parameters()).device\n        # Initialize hidden and cell states with zeros on the given device\n        h0 = torch.zeros(self.no_layers, batch_size, self.hidden_dim).to(device)\n        c0 = torch.zeros(self.no_layers, batch_size, self.hidden_dim).to(device)\n        return (h0, c0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:54:48.842228Z","iopub.execute_input":"2025-12-03T14:54:48.842537Z","iopub.status.idle":"2025-12-03T14:54:48.849303Z","shell.execute_reply.started":"2025-12-03T14:54:48.842491Z","shell.execute_reply":"2025-12-03T14:54:48.848584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 768  # mBERT hidden size\nhidden_dim = 64\noutput_dim = 3       # 3-class classification\nno_layers = 2\ndrop_prob = 0.4\n\nmodel = SentimentRNN_PretrainedEmbedding(no_layers, embedding_dim, hidden_dim, output_dim, drop_prob)\nmodel.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:54:59.525313Z","iopub.execute_input":"2025-12-03T14:54:59.525677Z","iopub.status.idle":"2025-12-03T14:54:59.534966Z","shell.execute_reply.started":"2025-12-03T14:54:59.525655Z","shell.execute_reply":"2025-12-03T14:54:59.534436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combine RNN with token-level embeddings","metadata":{}},{"cell_type":"code","source":"def train(model, loader):\n    model.train()\n    total_loss = 0\n    for x, y in loader:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:55:03.218229Z","iopub.execute_input":"2025-12-03T14:55:03.218944Z","iopub.status.idle":"2025-12-03T14:55:03.222754Z","shell.execute_reply.started":"2025-12-03T14:55:03.21892Z","shell.execute_reply":"2025-12-03T14:55:03.222136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, loader, y_true):\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for x, _ in loader:\n            x = x.to(DEVICE)\n            out = model(x)\n            pred = torch.argmax(out, dim=1)\n            preds.extend(pred.cpu().numpy())\n    print(classification_report(y_true, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:55:06.487866Z","iopub.execute_input":"2025-12-03T14:55:06.488102Z","iopub.status.idle":"2025-12-03T14:55:06.492499Z","shell.execute_reply.started":"2025-12-03T14:55:06.488086Z","shell.execute_reply":"2025-12-03T14:55:06.491828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get mBERT token embeddings for each language\nX_train_emb = encode_token_embeddings(x_train_text)      # shape: (N, 128, 768)\nX_val_en_emb = encode_token_embeddings(x_val_en_text)\nX_val_fr_emb = encode_token_embeddings(x_val_fr_text)\nX_val_zh_emb = encode_token_embeddings(x_val_zh_text)\nX_test_emb = encode_token_embeddings(x_test_text)\n\n# Convert labels to tensors\ny_train_tensor = torch.tensor(y_train)\ny_val_en_tensor = torch.tensor(y_val_en)\ny_val_fr_tensor = torch.tensor(y_val_fr)\ny_val_zh_tensor = torch.tensor(y_val_zh)\ny_test_tensor = torch.tensor(y_test)\n\n# Construct dataloaders\ntrain_loader = make_loader(X_train_emb, y_train_tensor, shuffle=True)\nval_loader_en = make_loader(X_val_en_emb, y_val_en_tensor)\nval_loader_fr = make_loader(X_val_fr_emb, y_val_fr_tensor)\nval_loader_zh = make_loader(X_val_zh_emb, y_val_zh_tensor)\ntest_loader = make_loader(X_test_emb, y_test_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:55:10.10141Z","iopub.execute_input":"2025-12-03T14:55:10.102287Z","iopub.status.idle":"2025-12-03T14:56:14.39098Z","shell.execute_reply.started":"2025-12-03T14:55:10.102261Z","shell.execute_reply":"2025-12-03T14:56:14.390369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 10\nclip = 5  # Gradient clipping threshold\nvalid_loss_min = np.inf  # Track minimum validation loss for saving best model\n\n# Initialize lists to store losses and accuracies\nepoch_tr_loss, epoch_tr_acc = [], []\nepoch_val_loss_fr, epoch_val_loss_en, epoch_val_loss_zh = [], [], []\nepoch_val_acc_fr, epoch_val_acc_en, epoch_val_acc_zh = [], [], []\n\n# Dictionary of validation loaders for different languages\nvalid_sets = {\n    \"French\": val_loader_fr,\n    \"English\": val_loader_en,\n    \"Chinese\": val_loader_zh\n}\n\nfor epoch in range(epochs):\n    # ===== Training =====\n    train_losses = []\n    train_correct = 0\n    train_total = 0\n    model.train()  # Set model to training mode\n\n    for inputs, labels in train_loader:\n        current_batch_size = inputs.size(0)\n        # Initialize hidden state for current batch and move to device\n        h = model.init_hidden(current_batch_size, device=inputs.device)\n        h = tuple([each.data.to(DEVICE) for each in h])\n    \n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE).long()\n    \n        optimizer.zero_grad()\n        output, h = model(inputs, h)\n    \n        loss = criterion(output, labels)\n        loss.backward()\n    \n        # Clip gradients to prevent exploding gradients\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n    \n        train_losses.append(loss.item())\n\n        # Calculate number of correct predictions per batch\n        _, preds = torch.max(output, 1)\n        train_correct += (preds == labels).sum().item()\n        train_total += labels.size(0)\n\n    # Calculate average training loss and accuracy for the epoch\n    epoch_train_loss = np.mean(train_losses)\n    epoch_train_acc = train_correct / train_total\n\n    # Record training loss and accuracy\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n\n    # ===== Validation =====\n    model.eval()  # Set model to evaluation mode\n    epoch_val_loss = {}\n    epoch_val_acc = {}\n\n    with torch.no_grad():\n        # Evaluate on each validation set (French, English, Chinese)\n        for lang, loader in valid_sets.items():\n            val_losses = []\n            val_correct = 0\n            val_total = 0\n\n            for inputs, labels in loader:\n                current_batch_size = inputs.size(0)\n                val_h = model.init_hidden(current_batch_size, device=inputs.device)\n                val_h = tuple([each.data.to(DEVICE) for each in val_h])\n\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE).long()\n\n                output, val_h = model(inputs, val_h)\n                val_loss = criterion(output, labels)\n                val_losses.append(val_loss.item())\n\n                _, preds = torch.max(output, 1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n\n            # Calculate average validation loss and accuracy for current language\n            epoch_val_loss[lang] = np.mean(val_losses)\n            epoch_val_acc[lang] = val_correct / val_total\n\n    # Record validation losses and accuracies per language\n    epoch_val_loss_fr.append(epoch_val_loss[\"French\"])\n    epoch_val_loss_en.append(epoch_val_loss[\"English\"])\n    epoch_val_loss_zh.append(epoch_val_loss[\"Chinese\"])\n\n    epoch_val_acc_fr.append(epoch_val_acc[\"French\"])\n    epoch_val_acc_en.append(epoch_val_acc[\"English\"])\n    epoch_val_acc_zh.append(epoch_val_acc[\"Chinese\"])\n\n    # ===== Logging =====\n    print(f\"Epoch {epoch + 1}\")\n    print(f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc*100:.2f}%\")\n    for lang in valid_sets.keys():\n        print(f\"Val Loss ({lang}): {epoch_val_loss[lang]:.4f} | Val Acc ({lang}): {epoch_val_acc[lang]*100:.2f}%\")\n    print(\"=\"*50)\n\n    # ===== Save best model based on French validation loss =====\n    if epoch_val_loss[\"French\"] <= valid_loss_min:\n        print(f\"Validation loss (French) decreased ({valid_loss_min:.6f} --> {epoch_val_loss['French']:.6f}). Saving model ...\")\n        torch.save(model.state_dict(), 'state_dict.pt')\n        valid_loss_min = epoch_val_loss[\"French\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:58:25.701547Z","iopub.execute_input":"2025-12-03T14:58:25.702113Z","iopub.status.idle":"2025-12-03T14:58:50.522451Z","shell.execute_reply.started":"2025-12-03T14:58:25.702088Z","shell.execute_reply":"2025-12-03T14:58:50.521773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plot accuracy and loss","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 6))\n\n# Plot Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Accuracy')\nplt.plot(epoch_val_acc_fr, label='Validation Accuracy (French)')\nplt.plot(epoch_val_acc_en, label='Validation Accuracy (English)')\nplt.plot(epoch_val_acc_zh, label='Validation Accuracy (Chinese)')\nplt.title(\"Accuracy over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\n\n# Plot Loss\nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train Loss')\nplt.plot(epoch_val_loss_fr, label='Validation Loss (French)')\nplt.plot(epoch_val_loss_en, label='Validation Loss (English)')\nplt.plot(epoch_val_loss_zh, label='Validation Loss (Chinese)')\nplt.title(\"Loss over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:59:49.553391Z","iopub.execute_input":"2025-12-03T14:59:49.553921Z","iopub.status.idle":"2025-12-03T14:59:49.92444Z","shell.execute_reply.started":"2025-12-03T14:59:49.553899Z","shell.execute_reply":"2025-12-03T14:59:49.923884Z"}},"outputs":[],"execution_count":null}]}